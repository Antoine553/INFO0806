---
title: "INFO0806 – Statistiques appliquées"
author: "FUZELLIER Maxence, BARBET Antoine"
date: "`r format(Sys.time(), '%d %B, %Y')`"
language: R
cran: http://cran.rstudio.com
output: 
  html_document:
    toc: true # table of content true
    toc_depth: 3 # three depths of headings (#, ## and ###)
editor_options: 
  chunk_output_type: inline
---

<style>
body {
  text-align: justify;
  font-size: 12pt;
}
</style>

<body>

******

<center>
**↓↓ Accès au projet ↓↓**
<br />
[Lien GitHub du projet](https://github.com/Antoine553/INFO0806)
</center>

******

# **Introduction**

Dans le cadre du module de statistiques appliquées, nous avions abordé les bases des statistiques et pris en main des outils permettant d'effectuer des opérations statistiques avancées grâce au langage R. Ainsi, notre choix s'est porté sur des données statistiques relatives au prestige (score attribuée) de différentes professions canadiennes.

Ce projet de statistiques appliquées consiste en l'utilisation d'outils R adaptés afin d'effectuer une modélisation statistique sur notre jeu de données et produire un rapport.

<br />

******

# **Outils et environnement de travail**


## R et RStudio 
 
R est un langage de programmation dont le but est de pouvoir traiter et organiser des jeux de données afin de pouvoir y appliquer des tests statistiques plus ou moins complexes et se représenter ces données graphiquement à l'aide d'une grande variété de graphiques disponibles. RStudio est une application proposant un environnement de développement et des outils adaptés au langage et à l'environnement de programmation R. 

La fonction principale de RStudio consiste à faciliter le développement d'applications en langage R. Pour ce faire, le programme dispose de nombreux outils qui vous permettent notamment de créer des scripts, compiler du code, créer des graphes, ainsi que de travailler avec divers jeux de données. 

<br />

## R markdown

L’extension R markdown permet de générer des documents de manière dynamique en mélangeant texte mis en forme et résultats produits par du code R. Les documents générés peuvent être au format HTML, PDF, Word, et bien d’autres. C’est donc un outil très pratique pour l’exportation, la communication et la diffusion de résultats d’analyse.

<br />

## Outils complémentaires 

* *tidyverse* : Il s’agit d’une collection d’extensions relatives à la science des données, et permettant la manipulation des tableaux de données, l’import/export de données, la manipulation de variables ou, entre autres, la visualisation de données ;
* *data.table* : Extension et manipulation avancée des tableaux de données ;
* *plotly* : Comme *ggplot2* (compris dans le *tidyverse*), ce package permet la visualisation de données, à la différence près qu’il renden ces graphiques interactifs ;
* *car* : Appliquer une régression linéaire et des tests statistiques ;
* *hrbrthemes* : Thèmes pour *ggplot2*.

<br />

******

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r load, message=FALSE, warning=FALSE, include=FALSE}
library(rmarkdown)
library(knitr)
library(tidyverse)
library(hrbrthemes)
library(plotly)
library(gridExtra)
library(GGally)
library(car)
```

# **Données**

Notre [jeu de données](https://github.com/selva86/datasets/blob/master/Prestige.csv) comporte 102 individus décrits par 6 variables :

* education : Nombre moyen d'années d'études des titulaires de postes, en 1971 ;
* income : Le revenu moyen des titulaires de postes, en dollars, en 1971 ;
* women : pourcentage de femmes dans la profession ; 
* prestige : Score de prestige Pineo-Porter pour la profession, d'après une enquête sociale réalisée au milieu des années 60 ;
* census : Le code de classification nationale de chaque profession ;
* type : Type de profession. Mangériale et technique (prof), col blanc (wc), col bleu (bc).

```{r datasets, include=FALSE}
data <- read_csv("data/prestige.csv") %>% as_tibble()
```
<br />

```{r show_data}
head(data) %>% paged_table()
```

## Visualisation des données

```{r viz_edu, fig.height=6, fig.width=9, message=FALSE, warning=FALSE}
viz_edu <- data %>% 
  ggplot(aes(x = prestige, y = education, col = type)) + 
  geom_point() + theme_bw() +
  scale_x_continuous(
    breaks = seq(25, 75, 25)
  ) +
  scale_y_continuous(
    breaks = seq(6, 16, 2)
  )

viz_edu <- ggplotly(viz_edu)

viz_inc <- data %>% 
  ggplot(aes(x = prestige, y = income, col = type)) + 
  geom_point() + theme_bw() +
  scale_x_continuous(
    breaks = seq(25, 75, 25)
  ) +
  scale_y_continuous(
    breaks = seq(0, 20000, 10000)
  )

viz_inc <- ggplotly(viz_inc)

viz_women <- data %>% 
  ggplot(aes(x = prestige, y = women, col = type)) + 
  geom_point() + theme_bw() +
  scale_x_continuous(
      breaks = seq(25, 75, 25)
    ) +
    scale_y_continuous(
      breaks = seq(0, 100, 25)
    )

viz_women <- ggplotly(viz_women)

viz_census <- data %>% 
  ggplot(aes(x = prestige, y = census, col = type)) + 
  geom_point() + theme_bw() +
  scale_x_continuous(
    breaks = seq(25, 75, 25)
  ) +
  scale_y_continuous(
    breaks = seq(2500, 7500, 2500)
  )
  
viz_census <- ggplotly(viz_census)

subplot(viz_edu, viz_inc, viz_women, viz_census, nrows = 2, margin = 0.07)
```

Dans un premier temps, nous pouvons observer une forte relation linéaire entre *prestige* et les variables *education* et *income*, contrairement à *women* et *census*.

Dans un second temps, nous pouvons étudier d'éventuelles corrélations entre deux variables à l'aide d'un nuage de points. Ici, nous utilisons la fonction *ggpairs* afin de pouvoir en observer plusieurs en même temps.

```{r scatterM, fig.height=6, fig.width=9, message=FALSE, warning=FALSE}
generate_scatterM <- function(data, mapping){
  data %>% ggplot(mapping = mapping) + geom_point(size = 0.8) + 
    geom_smooth(method = loess, color = "red", size = 0.85, se = FALSE) +
    geom_smooth(method = lm, color = "blue", size = 0.85, se = FALSE)
}

scatterM <- data %>% ggpairs(columns = 1:5, lower = list(continuous = generate_scatterM))

scatterM <- ggplotly(scatterM)
scatterM
```

Avec les observations précédentes, nous pouvons déjà écarter certaines linéarités et choisir lesquelles sont intéressantes à étudier. Ici, nous reprenons donc la relation entre la variable *prestige* (il s’agit d’un score de prestige relatif à la profession) et la variable *education* (qui reflète le niveau d’étude). Cette fois nous utilisons *ggplot2* pour une meilleure représentation.

```{r education, fig.height=6, fig.width=9, message=FALSE, warning=FALSE}
education <- data %>% 
  ggplot(aes(x = education, y = prestige)) +
  geom_point() +
  geom_smooth(method = loess, se = T) +
  geom_smooth(method = lm, color = "red", se = F) +
  theme_bw() +
  scale_x_continuous(
    breaks = seq(6, 16, 2)
  ) +
  scale_y_continuous(
    breaks = seq(20, 80, 20)
  )

scatter_edu <- ggplotly(education)
scatter_edu
```

La droite bleue est définie par la méthode des moindres carrés (MSE), il s'agit d'une droite de régression linéaire entre les variables *education* et *prestige*.

La courbe rouge indique la tendance globale entre ces deux variables, il s'agit d'une courbe de régression de type lowess. Les deux courbes extérieures représentent l'intervalle de confiance de cette courbe de régression.

On constate que que la droite de régression est presque toujours comprise dans l’intervalle de confiance, l'hypothèse de linéarite entre les variables *education* et *prestige* est donc acceptable.

<br />

Nous testons aussi la relation entre les variables *income* et *prestige*.

```{r income, fig.height=6, fig.width=9, message=FALSE, warning=FALSE}
income <- data %>% 
  ggplot(aes(x = income, y = prestige)) +
  geom_point() +
  geom_smooth(method = loess, se = T) +
  geom_smooth(method = lm, color = "red", se = F) +
  theme_bw() +
  scale_x_continuous(
    breaks = seq(0, 25000, 5000)
  ) +
  scale_y_continuous(
    breaks = seq(20, 100, 20)
  )

scatter_inc <- ggplotly(income)
scatter_inc
```

Ici, en regardant la forme du lien entre la variable entre les deux variables, on s’aperçoit que la droite de régression suis bien moins l’intervalle de confiance de la courbe lowess. l’hypothèse de linéarité est alors plus critiquable.

Pour déterminer la droite de régression, on ajuste un modèle linéaire simple aux données, à l’aide de la fonction “lm”.

```{r lin_model_edu}
lin_model_edu <- lm(prestige ~ education, data = data)
lin_model_edu
```

« Intercept » correspond ici à l’ordonnée à l’origine, le « b » de notre droite, et le « x » est la pente de la droite, ce qui correspond au « b » dans notre notation.

L’équation, de notre droite est donc $y = -10.732 + 5.361x$.

<br />

# **Évaluation des résultats**

Il existe différentes manières, entre diagrammes et tests statistiques, visant à évaluer le lien linéaire entre deux variables. Ce lien est dit « significatif » s'il remplit certaines conditions. En effet, **les résidus doivent être indépendants, distribués selon une loin Normale de moyenne 0 et de façon homogène (variance constante).**

<br />

## Indépendance et auto-corrélation

### Visualisation

En premier lieu, il faut évaluer l’hypothèse d’indépendance des résidus. À l'aide d'un *lag plot*, il possible de mettre en évidence la présence d'une auto-corrélation.

```{r lagplot, fig.height=6, fig.width=9, message=FALSE, warning=FALSE}
bacf <- acf(residuals(lin_model_edu), plot = F)
bacfdf <- with(bacf, data.frame(lag, acf))
conf.level <- 0.95
ciline <- qnorm((1 - conf.level)/2)/sqrt(length(residuals(lin_model_edu)))

lag_plot <- bacfdf %>%
  ggplot(aes(x = lag, y = acf)) +
  geom_hline(aes(yintercept = 0)) +
  geom_segment(aes(xend = lag, yend = 0)) +
  geom_hline(aes(yintercept = ciline), linetype = 3, color = 'darkblue') +     
  geom_hline(aes(yintercept = -ciline), linetype = 3, color = 'darkblue') +
  theme_bw()

lag_plot <- ggplotly(lag_plot, tooltip = c("lag", "acf"))
lag_plot
```

Sur le graphique ci-dessus, chaque trait correspond à un *lag*, ou coefficient de corrélation entre les résidus de chaque point, les pointillés bleus quant à eux sont les intervalles de confiance du coefficient de corrélation égale à 0.

En s'appuyant sur ce graphique, nous pouvons observer qu'une auto-corrélation significative est présente pour les lags 1 à 3, 5, 7 et 12 par exemple.

### Test de Durbin-Watson

Afin d'aller plus loin et de détecter une éventuelle auto-corrélation des erreurs d'ordre 1 (lag = 1), il est possible d'emloyer **le test de Durbin-Watson**.

L'intérêt d'avoir recours à ce test est de déterminer si le modèle est perfectible. Souvent, l’autocorrélation s’observe sur les résidus d’une modélisation de série chronologique. Toutefois, si le type de modèle est bien choisi (en l'occurrence une régrassion linéaire simple), il peut exister une véritable autocorrélation entre les observations. En revanche, si aucune autocorrélation n'a de raison d'être, il faut chercher d’autres variables candidates.

```{r durbinWatson_edu}
durbinWatsonTest(lin_model_edu)
```

Le test de Durbin-Watson indique qu’il existe une auto-corrélation significative entre les résidus d’une ligne du tableau de données et ceux de la ligne suivante.

<br />

## Normalité

### Visualisation

La deuxième condition susmentionnée concerne la distribution des résidus selon une loi Normale de moyenne 0. La pertinence de l'ajustement d'une distribution donnée à un modèle peut être représentée graphiquement grâce à un diagramme quantile-quantile, ou *QQ plot*.

```{r qq_edu, fig.height=6, fig.width=9, message=FALSE, warning=FALSE}
qq_edu <- lin_model_edu %>%
  ggplot(aes(sample = rstandard(.))) +
  stat_qq_line(color = "red", linetype = "dashed") +
  stat_qq(size = 1.2) +
  theme_bw() +
  scale_x_continuous(
    breaks = seq(-2, 2, 1)
  ) +
  scale_y_continuous(
    breaks = seq(-3, 3, 1)
  ) +
  labs(title = "Normal Q-Q",
       x = "Theoretical Quantiles\nlm(prestige ~ education)", 
       y = "Standardized residuals")

qq_edu <- ggplotly(qq_edu)
qq_edu
```

Nous constatons que les résidus sont relativement bien alignés le long de la droite figurant sur le graphique, nous pouvons donc en conclure que la distribution de cette série de données suit une loi normale.

### Test de Shapiro-Wilk

En statistiques, le **test de Shapiro–Wilk** teste l'hypothèse nulle selon laquelle un échantillon de données est issu d'une population normalement distribuée. Ce test peut être employé afin d'évaluer l'hypothèse de normalité des résidus.

```{r shapiro_edu}
shapiro.test(residuals(lin_model_edu))
```

On considère que cette hypothèse est rejetée si la p-valeur est inférieure à $0,05$, mais dans le cas présent le test accepte la normalité.

<br />

Il est essentiel d'appliquer des tests statistiques tels que celui de Shapiro-Wilk. Un exemple parlant serait celui d'une régression linéaire simple entres les variables *income* et *prestige*.

```{r lin_model_inc}
lin_model_inc <- lm(prestige~income, data = data)
lin_model_inc
```

```{r qq_inc, fig.height=6, fig.width=9, message=FALSE, warning=FALSE}
qq_inc <- lin_model_inc %>%
  ggplot(aes(sample = rstandard(.)))+
  stat_qq_line(color = "red", linetype = "dashed") +
  stat_qq(size = 1.2) +
  theme_bw() +
  scale_x_continuous(
    breaks = seq(-2, 2, 1)
  ) +
  scale_y_continuous(
    breaks = seq(-3, 3, 1)
  ) +
  labs(title = "Normal Q-Q",
       x = "Theoretical Quantiles\nlm(prestige ~ income)", 
       y = "Standardized residuals")
qq_inc <- ggplotly(qq_inc)
qq_inc
```

L'alignement des résidus est légèrement moins précis que sur le **QQ plot** entre les variables *education* et *prestige*, mais il semble plutôt satisfaisant. On ne constaste aucune valeur aberrante.

```{r shapiro_inc}
shapiro.test(residuals(lin_model_inc))
```

Cependant, le test de Shapiro-Wilk renvoie une p-valeur inférieure à $0,05$ et rejette donc l'hypothèse de normalité.

<br />

## Homoscédasticité

### Visualisation

Enfin, le concept d'homoscédasticité est utilisé dans le contexte de la régression linéaire pour décrire le cas où la variance des erreurs du modèle est la même pour toutes les observations. Autrement dit, les variances sont homogènes et les erreurs identiquement distribuées. Il s'agit de l'une des propriétés fondamentales du modèle de régression linéaire.

Afin de visualiser cela, nous avons besoin des valeurs prédites par le modèle (*fitted values*).

Dans notre cas, le graphique représentera les valeurs de *prestige* prédite par le modèle pour les valeurs de *education* présentes dans les données.

```{r sc_loc_edu, fig.height=6, fig.width=9, message=FALSE, warning=FALSE}
sc_loc_edu <- lin_model_edu %>%
  ggplot(aes(fitted(.), sqrt(abs(rstandard(.))))) + 
  geom_point(size = 1.2) +
  geom_smooth(method = loess, se = FALSE) +
  theme_bw() +
  scale_x_continuous(
    breaks = seq(30, 70, 10)
  ) +
  scale_y_continuous(
    limits = c(0, 1.5),
    breaks = seq(0, 1.5, 0.5)
  ) +
  labs(title = "Scale-location",
       x = "Fitted values\nlm(prestige ~ education)", 
       y = "sqrt |Standardized residuals|")

sc_loc_edu <- ggplotly(sc_loc_edu)
sc_loc_edu
```

Le graphique nous montre que les résidus sont répartis de façon homogène le long du gradient des valeurs prédites. Ceci est mis en évidence par la courbe de régression locale qui est quasiment plate.

### Test de Breusch-Pagan

**Le test de Breusch-Pagan** permet de tester l'hypothèse d'homoscédasticité du terme d'erreur d'un modèle de régression linéaire. Si la variance est constante, alors on a de l'homoscédasticité ; en revanche, si elle va varie, on a de l'hétéroscédasticité.

```{r ncv_test}
ncvTest(lin_model_edu)
```

La p-valeur associée au test est supérieure au seuil de $0.05$, ainsi l'hypothèse d'homoscédasticité est acceptée.

<br />

## Linéarité

### Visualisation

Afin de vérifier l'hypothèse selon laquelle la relation entre les variables *education* et *prestige* est linéaire, ainsi que l'homoscédasticité, il est possible de générer un *residuals vs. fitted plot*.

```{r res_v_fit, fig.height=6, fig.width=9, message=FALSE, warning=FALSE}
res_v_fit_edu <- lin_model_edu %>%
  ggplot(aes(fitted(.), residuals(.))) + 
  geom_point() +
  stat_smooth(method="loess", se = FALSE) + 
  geom_hline(yintercept = 0, col = "red", linetype = "dashed") +
  theme_bw() +
  scale_x_continuous(
    breaks = seq(30, 70, 10)
  ) +
  scale_y_continuous(
    limits = c(-30, 20),
    breaks = seq(-30, 20, 10)
  ) +
  labs(title = "Residuals vs. Fitted values",
       x = "Fitted values\nlm(prestige ~ education)", 
       y = "Residuals")
    
res_v_fit_edu <- ggplotly(res_v_fit_edu)
res_v_fit_edu
```

Il n'y a pas de schéma clair dans les données, ni d'aberrations évidentes. Les résidus sont, à peu de choses près, uniformément distribués le long de la ligne 0. La droite de régression est bien adaptée aux données, par conséquent l’hypothèse de linéarité est acceptable.

<br />

# **Interprétation des résultats**

En s'appuyant sur les différentes représentations graphiques ainsi que les tests effectués précédémment, on peut en conclure que les propriétés fondamentales de la régression linéaire (indépendance des résidus, normalité, homoscédasticité et linéarité) sont satisfaites.

Partant de ce postulat, les résultats de la régression linéaire entre les variables *education* et *prestige* peuvent être interprétés.

```{r summ_model}
summary(lin_model_edu)
```

La fonction Summary nous permet d'avoir un bon apercus de ses resultats en nous donnant deux elements. Nous avons dans la première partie nomée Residuals, la norme des residus (min, max, quartile et mediane).
Dnas la seconde partie Coefficients nous avons deux lignes. La première ligne de la partie coefficients concerne l’ordonnée à l’origine, alors que la seconde ligne concerne la pente. Le plus important ici est le coefficient de la pente égal à 5.361.

<br />

## Intervale de confiance

La fonction “confint” permet d’obtenir facilement ces intervalles de confiance. Ils sont basés sur la loi de Student.
```{r confint_model}
confint(lin_model_edu) %>% kable()
```
l’intervalle de confiance à 97.5% de la pente est une étendue de valeurs qui a une probabilité de 97.5% de contenir la vraie pente.

<br />

## Prédictions

```{r preds}
predictions <- tibble(education=c(9.21, 11.07, 14.62))
predict(lin_model_edu, newdata = predictions, interval = "confidence") %>% kable()
```

<br />

## Visualisation des composants

```{r add_res}
data$residuals <- residuals(lin_model_edu)
head(data) %>% kable()
```

```{r add_fitted}
data$fitted <- fitted(lin_model_edu)
head(data) %>% kable()
```

```{r covariance}
vcov(lin_model_edu) %>% kable()
```

<br />

# **Représentation finale de la régression**

```{r add_pred_intervals, message=FALSE, warning=FALSE}
pred_interval <- predict(lin_model_edu, interval = "prediction")
data <- cbind(data, pred_interval)
```

```{r lin_regression, fig.height=6, fig.width=9, message=FALSE, warning=FALSE}
linear_regression <- data %>% 
  ggplot(aes(y = prestige, x = education)) +
  geom_point(size = 1.2) +
  geom_smooth(color = "red", method = "lm", fill = "red") +
  geom_line(aes(y = lwr), color = "red", linetype = "dashed") +
  geom_line(aes(y =  upr), color = "red", linetype = "dashed") +    
  annotate("text", x = 9, y = 80, 
           label = paste0("prestige = ", coef(lin_model_edu)["(Intercept)"] %>% round(3), 
                          " + ", coef(lin_model_edu)["education"] %>% round(3), " * education")) +
  theme_bw() +
  scale_x_continuous(
    breaks = seq(6, 16, 2)
  ) +
  scale_y_continuous(
    breaks = seq(25, 75, 25)
  ) +
  labs(title = "Linear Regression",
       x = "Education", y = "Prestige")
  

linear_regression <- ggplotly(linear_regression)
linear_regression
```

<br />

# **Bilan**

<br />

Le modèle de régression linéaire est aussi bien utilisé pour chercher à prédire un phénomène que pour chercher à l'expliquer. Après avoir estimé un modèle de régression linéaire, on peut prédire quel serait le niveau de y pour des valeurs particulières de x. Il permet également d'estimer l'effet d'une ou plusieurs variables sur une autre en contrôlant par un ensemble de facteurs. En apprentissage statistique, la méthode de régression linéaire est considérée comme une méthode d'apprentissage supervisé utilisée pour prédire une variable quantitative. Dans cette perspective, on entraîne généralement le modèle sur un échantillon d'apprentissage et on teste ensuite les performances prédictives du modèle sur un échantillon de test. 

<br />

Ce projet a été réalisé dans le cadre du cours de statistiques appliquées. Celui-ci convoque des connaissances en statistiques acquise plus tot cette année et fait le lien avec notre module d'apprentissage supervisé. En effet afin de concevoir nos systèmes prévisionnels, comprendre comment les relations entre les variables d'un jeu de donnée peuvent être étudier est une étape primordial. Il a été l’occasion pour nous, étudiants, de consolider nos compétences en analyse et manipulation de données, puis d'en acquérir de nouvelles par le biais de ce projet.

<br />
<br />
<br />

</body>